experiment_tag: 'comparable_baselines'

seed: 42
wandb_mode: 'disabled'

model:
  type: gru
  enable_benchmarking: True

  setup:
    pre_trained_path: ~
    input_size: 100
    output_size: 2
    memory: 10 # only used with MLP

    mlp_cfg:
      _target_: models.baselines.ConfigurableMLP
      dims: ["${model.setup.input_size}",256,512,1024,1024,512,256,"${model.setup.output_size}"]
      p_drop: 0.1
      nonlinearity: ReLU
    
    rnn_cfg:
      _target_: models.baselines.ConfigurableRecurrentModel
      n_blocks: 4
      block_cfg:
        rec_type: RNN
        input_dim: ${model.setup.input_size}
        hidden_dim: 212
        n_rec_layers: 8
        output_dim: ${model.setup.output_size}
        p_drop: 0.1
        bidirectional: False
    
    gru_cfg:
      _target_: models.baselines.ConfigurableRecurrentModel
      n_blocks: 4
      block_cfg:
        rec_type: GRU
        input_dim: ${model.setup.input_size}
        hidden_dim: 164
        n_rec_layers: 4
        output_dim: ${model.setup.output_size}
        p_drop: 0.1
        bidirectional: False
    
    lstm_cfg:
      _target_: models.baselines.ConfigurableRecurrentModel
      n_blocks: 4
      block_cfg:
        rec_type: LSTM
        input_dim: ${model.setup.input_size}
        hidden_dim: 162
        n_rec_layers: 4
        output_dim: ${model.setup.output_size}
        p_drop: 0.1
        bidirectional: False

    # CausalHomogenizer:
    #   num_latents: 128
    #   latent_dim: 32
    #   input_dim: 32
    #   qk_out_dim: ~
    #   v_out_dim: ~
    #   num_cross_attn_heads: 1
    #   cross_attn_widening_factor: 4
    #   use_query_residual: True
    #   cross_attn_lin_dropout: 0.3
    #   cross_attention_dropout: 0.1
    #   num_virtual_channels: ${model.setup.input_size}
tokenizer:
  latent_step: 0.01
  num_latents_per_step: 1
  use_memory_efficient_attn: False
dropout:
  min_units: 30
  mode_units: 100
  max_units: 300
  peak: 4
  M: 10
  max_attempts: 100
sampler:
  max_time: 1.0 # CHANGE FOR RTT 0.6
  window_length: 1 # CHANGE FOR RTT 0.599
  batch_size: 128
train:
  num_epochs: 10
  lr: 0.002
  weight_decay: 0.0001
data:
  ## ORIGINAL
  data_path: '/mnt/kostas-graid/datasets/ioannis/bio_spikes/all_h5_files/'
  dandiset: '000688'
  lock_data_idcs: ~

  # data_path: '/mnt/kostas-graid/datasets/ioannis/bio_spikes/other_datasets/monkey_T/h5_files/'
  # dandiset: '000688'
  # lock_data_idcs: ~
  
  ## MC MAZE (second line is with subsampling)
  # data_path: '/mnt/kostas-graid/datasets/ioannis/bio_spikes/other_datasets/mc_maze/all_h5_files/'
  # data_path: '/mnt/kostas-graid/datasets/ioannis/bio_spikes/other_datasets/mc_maze/subsampled_h5_files/'
  # dandiset: '000128'
  # lock_data_idcs: [0]

  ## MC RTT
  # data_path: '/mnt/kostas-graid/datasets/ioannis/bio_spikes/other_datasets/mc_rtt/all_h5_files/'
  # dandiset: '000129'
  # lock_data_idcs: ~
save_path: "trained_models/"